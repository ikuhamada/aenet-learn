 ======================================================================
                       Training process started.                       
 ======================================================================
                                                                       
                          2021-02-10  14:57:19                         
                                                                       
 
 Copyright (C) 2015-2018 Nongnuch Artrith and Alexander Urban
 
 This program is distributed in the hope that it will be useful, but
 WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 Mozilla Public License, v. 2.0, for more details.
 
 ----------------------------------------------------------------------
                              Parallel run                             
 ----------------------------------------------------------------------
 
 Number of processes : 1
 
 ----------------------------------------------------------------------
                       Training set normalization                      
 ----------------------------------------------------------------------
 
 The training set will be normalized now.  Depending on its size this
 process can take a while.  The normalized data set will be written to
 another file. Load that file in future to avoid this step.
 
 Name of the new training set file: H2O.train.scaled
 
 The network output energy will be normalized to the interval [-1,1].
   Energy scaling factor: f = 55.962441
   Atomic energy shift  : s = -3.455197
 
 Training set normalization done.
 
 ----------------------------------------------------------------------
                                Networks                               
 ----------------------------------------------------------------------
 
 Creating a new H network
 
 Number of layers :   4
 
 Number of nodes (without bias) 
 and activation type per layer :
 
       1 :    27
       2 :    10  hyperbolic tangent (tanh)
       3 :    10  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)
 
 Required memory (words) :      48919 (    382.18 KB)
 
 Total number of weights (incl. bias) :      401
 
 Creating a new O network
 
 Number of layers :   4
 
 Number of nodes (without bias) 
 and activation type per layer :
 
       1 :    30
       2 :    10  hyperbolic tangent (tanh)
       3 :    10  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)
 
 Required memory (words) :      65443 (    511.27 KB)
 
 Total number of weights (incl. bias) :      431
 
 Timing info will be written to: train.time
 
 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------
 
 Saving the H network to file : H.10t-10t.ann
 Saving the O network to file : O.10t-10t.ann
 
 ----------------------------------------------------------------------
                           Training set info.                          
 ----------------------------------------------------------------------
 
 Training set file                   : H2O.train.scaled
 Number of structures in the data set: 14
 
 Atomic species in training set      : 2
   Species : H O
 
 Average energy (eV/atom) : -0.063838
 Minimum energy (eV/atom) : -1.000000
 Maximum energy (eV/atom) : 1.000000
 
 The input and output values have been normalized to [-1.0, 1.0].
 Structures outside of this interval will not be used for training.
   Energy scaling factor: 55.962441
   Atomic energy shift  : -3.455197
 
 ----------------------------------------------------------------------
                            Training details                           
 ----------------------------------------------------------------------
 
 Training method         : Limited Memory BFGS
 
 Number of iterations    : 1000
 
 Training structures     : 13
 Testing  structures     : 1
 
 Testing set : 
 
       3 
 
 ----------------------------------------------------------------------
                            Training process                           
 ----------------------------------------------------------------------
 
 Weight optimization for 1000 epochs using the Limited Memory BFGS method.
 
 Sampling type               : sequential
 
        |------------TRAIN-----------|  |------------TEST------------|
 epoch             MAE          <RMSE>             MAE          <RMSE>
     0    1.305775E-02    1.499621E-02    2.505365E-02    2.505365E-02 <
     1    1.305775E-02    1.499621E-02    2.505365E-02    2.505365E-02 <
     2    7.824220E-03    9.582171E-03    1.074597E-03    1.074597E-03 <
     3    8.368408E-03    1.038165E-02    2.141188E-04    2.141188E-04 <
     4    7.142833E-03    8.385126E-03    5.615325E-04    5.615325E-04 <
     5    6.885177E-03    7.843463E-03    9.695954E-04    9.695954E-04 <
     6    5.619056E-03    6.266010E-03    2.011257E-03    2.011257E-03 <
     7    3.997868E-03    4.988706E-03    1.565274E-03    1.565274E-03 <
     8    3.625724E-03    4.760820E-03    2.645647E-03    2.645647E-03 <
     9    3.630670E-03    4.622094E-03    2.427981E-03    2.427981E-03 <
    10    3.533557E-03    4.464650E-03    1.837496E-03    1.837496E-03 <
    11    3.266388E-03    4.042829E-03    1.228731E-03    1.228731E-03 <
    12    3.001424E-03    3.769710E-03    1.374052E-03    1.374052E-03 <
    13    3.003047E-03    3.636211E-03    1.647191E-03    1.647191E-03 <
    14    2.999084E-03    3.554882E-03    1.825677E-03    1.825677E-03 <
    15    2.394269E-03    3.089582E-03    1.607142E-03    1.607142E-03 <
    16    2.321504E-03    3.025152E-03    1.277749E-03    1.277749E-03 <
    17    2.349211E-03    2.986055E-03    6.654088E-04    6.654088E-04 <
    18    2.307544E-03    2.932908E-03    5.794428E-04    5.794428E-04 <
    19    2.042407E-03    2.600539E-03    9.467740E-04    9.467740E-04 <
    20    1.968441E-03    2.547400E-03    1.067933E-03    1.067933E-03 <
    21    1.940597E-03    2.523240E-03    1.162880E-03    1.162880E-03 <
    22    1.933107E-03    2.492373E-03    1.251153E-03    1.251153E-03 <
    23    1.849801E-03    2.452015E-03    1.192414E-03    1.192414E-03 <
    24    2.000147E-03    2.631515E-03    2.176093E-03    2.176093E-03 <
    25    1.576187E-03    2.341843E-03    1.124042E-03    1.124042E-03 <
    26    1.940459E-03    2.620665E-03    6.680373E-04    6.680373E-04 <
    27    1.474285E-03    2.323206E-03    1.020838E-03    1.020838E-03 <
    28    1.493207E-03    2.317238E-03    7.661045E-04    7.661045E-04 <
    29    1.477944E-03    2.313818E-03    6.732102E-04    6.732102E-04 <
    30    1.458795E-03    2.311427E-03    5.683199E-04    5.683199E-04 <
    31    1.459870E-03    2.310214E-03    4.730177E-04    4.730177E-04 <
    32    1.451384E-03    2.308343E-03    3.921436E-04    3.921436E-04 <
    33    1.421618E-03    2.303678E-03    1.950186E-04    1.950186E-04 <
    34    1.396267E-03    2.299635E-03    2.509852E-05    2.509852E-05 <
    35    1.376405E-03    2.297564E-03    6.650048E-05    6.650048E-05 <
    36    1.368653E-03    2.296638E-03    3.095844E-05    3.095844E-05 <
    37    1.362420E-03    2.296064E-03    2.308009E-06    2.308009E-06 <
    38    1.357232E-03    2.295627E-03    3.737204E-06    3.737204E-06 <
    39    1.345934E-03    2.294777E-03    1.526397E-05    1.526397E-05 <
    40    1.330802E-03    2.294325E-03    6.894736E-05    6.894736E-05 <
    41    1.328060E-03    2.294273E-03    9.147120E-05    9.147120E-05 <
    42    1.328370E-03    2.294258E-03    1.006639E-04    1.006639E-04 <
    43    1.333325E-03    2.294257E-03    1.077752E-04    1.077752E-04 <
    44    1.330862E-03    2.294246E-03    1.042209E-04    1.042209E-04 <
    45    1.329669E-03    2.294222E-03    1.107732E-04    1.107732E-04 <
    46    1.327337E-03    2.294157E-03    1.207815E-04    1.207815E-04 <
    47    1.326631E-03    2.293973E-03    1.457371E-04    1.457371E-04 <
    48    1.323131E-03    2.293608E-03    1.935432E-04    1.935432E-04 <
    49    1.312472E-03    2.292969E-03    2.719979E-04    2.719979E-04 <
    50    1.317303E-03    2.293659E-03    3.019031E-04    3.019031E-04 <
    51    1.310582E-03    2.292789E-03    2.831570E-04    2.831570E-04 <
    52    1.297262E-03    2.292439E-03    3.588920E-04    3.588920E-04 <
    53    1.285857E-03    2.292176E-03    3.648593E-04    3.648593E-04 <
    54    1.277229E-03    2.292116E-03    3.816394E-04    3.816394E-04 <
    55    1.280294E-03    2.292130E-03    4.088967E-04    4.088967E-04 <
    56    1.275899E-03    2.292112E-03    3.905965E-04    3.905965E-04 <
    57    1.274465E-03    2.292110E-03    3.953167E-04    3.953167E-04 <
    58    1.274189E-03    2.292109E-03    3.956770E-04    3.956770E-04 <
    59    1.273914E-03    2.292109E-03    3.970968E-04    3.970968E-04 <
    60    1.273548E-03    2.292109E-03    3.995206E-04    3.995206E-04 <
    61    1.273435E-03    2.292109E-03    4.000117E-04    4.000117E-04 <
    62    1.273370E-03    2.292109E-03    4.000685E-04    4.000685E-04 <
    63    1.273348E-03    2.292109E-03    4.000273E-04    4.000273E-04 <
    64    1.273343E-03    2.292109E-03    4.000055E-04    4.000055E-04 <
    65    1.273343E-03    2.292109E-03    4.000055E-04    4.000055E-04 <
    66    1.273343E-03    2.292109E-03    4.000055E-04    4.000055E-04 <
    67    1.273343E-03    2.292109E-03    4.000055E-04    4.000055E-04 <
 The optimization has converged. Training stopped.
 
 
 Training finished.
 
 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------
 
 Saving the H network to file : H.10t-10t.ann
 Saving the O network to file : O.10t-10t.ann
 
                                                                       
                          2021-02-10  14:57:19                         
                                                                       
 ======================================================================
                     Neural Network training done.                     
 ======================================================================
